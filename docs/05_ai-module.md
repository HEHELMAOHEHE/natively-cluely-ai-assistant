## 5. Модуль искусственного интеллекта и обработки речи

Natively реализует многослойную систему ИИ, которая объединяет распознавание речи, семантический анализ, генерацию ответов и локальную систему знаний (RAG) для создания мощного ассистента в реальном времени.

### Архитектура модуля ИИ

#### Центральный компонент: IntelligenceManager
Класс `IntelligenceManager` является ядром интеллектуальной системы приложения. Он отвечает за:
- Обработку транскрипций из аудио-каналов
- Управление контекстом встречи
- Генерацию рекомендаций и ответов
- Интеграцию с RAG-системой для доступа к истории
- Создание сводок и вопросов для уточнения

```typescript
private intelligenceManager: IntelligenceManager = new IntelligenceManager(this.processingHelper.getLLMHelper())
```

#### События и коммуникация
`IntelligenceManager` использует систему событий для взаимодействия с интерфейсом:

```typescript
this.intelligenceManager.on('suggested_answer', (answer: string, question: string, confidence: number) => {
  win.webContents.send('intelligence-suggested-answer', { answer, question, confidence })
})

this.intelligenceManager.on('recap', (summary: string) => {
  win.webContents.send('intelligence-recap', { summary })
})
```

Основные события:
- `assist_update`: обновления анализа ситуации
- `suggested_answer`: предложенный ответ на вопрос
- `refined_answer`: уточненный или перегенерированный ответ
- `recap`: сводка встречи
- `follow_up_questions_update`: вопросы для уточнения
- `manual_answer_result`: результат ручного запроса пользователя
- `mode_changed`: изменение режима работы ИИ
- `error`: ошибки в работе ИИ

### Поддерживаемые модели ИИ

Natively поддерживает широкий спектр облачных и локальных моделей:

#### Облачные модели
| Модель | Описание |
|-------|--------|
| **Gemini 3 Pro/Flash** | Рекомендуемый вариант с огромным контекстным окном (2M токенов) и низкой стоимостью |
| **OpenAI GPT-5.2** | Высокие способности к рассуждению и анализу |
| **Anthropic Claude 4.5** | Отличное понимание кода и сложных задач |
| **Groq/Llama 3** | Ультрабыстрая работа (ответы почти мгновенно) |

#### Локальные модели через Ollama
Поддержка полностью автономной работы без интернета:
- Llama 3
- Mistral
- Gemma
- CodeLlama
- Любой другой OpenAI-совместимый эндпоинт

### Система провайдеров ИИ

Приложение позволяет использовать любой ИИ-провайдер через гибкую систему конфигурации:

#### Конфигурация через .env
```env
# Cloud AI
GEMINI_API_KEY=your_key
GROQ_API_KEY=your_key
OPENAI_API_KEY=your_key
CLAUDE_API_KEY=your_key

# Local AI (Ollama)
USE_OLLAMA=true
OLLAMA_MODEL=llama3.2
OLLAMA_URL=http://localhost:11434

# Default Model Configuration
DEFAULT_MODEL=gemini-3-flash-preview
```

#### Динамическое переключение моделей
Система поддерживает изменение моделей "на лету" во время встречи:
- Переключение между разными облачными провайдерами
- Переход с облачных моделей на локальные и обратно
- Автоматическое определение доступных моделей через Ollama API

### Обработка речи

#### Speech-to-Text провайдеры
Natively поддерживает множество систем преобразования речи в текст:

| Провайдер | Особенности |
|----------|-----------|
| **Google Cloud STT** | Через Service Account JSON, высокая точность |
| **Deepgram** | Потоковая транскрипция, низкая задержка |
| **Groq** | Ультрабыстрая обработка |
| **OpenAI Whisper** | Точное распознавание через API OpenAI |
| **ElevenLabs** | Продвинутое распознавание речи |
| **Azure Speech Services** | Сервисы распознавания от Microsoft |
| **IBM Watson** | Система распознавания речи от IBM |

#### Двойная система STT
Приложение использует два независимых канала STT:

```typescript
private googleSTT: GoogleSTT | RestSTT | DeepgramStreamingSTT | null = null; // для собеседников
private googleSTT_User: GoogleSTT | RestSTT | DeepgramStreamingSTT | null = null; // для пользователя
```

Это позволяет:
- Раздельно обрабатывать речь собеседников и пользователя
- Использовать разные провайдеры для разных каналов
- Задавать частные вопросы ИИ без участия в основной беседе

### Контекстное управление

#### Rolling Context Window
Система поддерживает "окно памяти" текущей встречи:
- Хранение последних N сообщений как контекста для ИИ
- Динамическое обновление контекста по мере развития диалога
- Приоритизация последних реплик для более релевантных ответов

#### Smart Scope Detection
Инновационная система определения контекста запроса:
- Автоматически определяет, относится ли вопрос к текущей встрече или истории
- Переключается между режимами "текущий разговор" и "поиск по прошлым встречам"
- Позволяет задавать вопросы типа "Что Джон говорил об API на прошлой неделе?"

### Локальная система знаний (RAG)

#### Полностью локальная векторная база данных
Все данные хранятся и обрабатываются исключительно на устройстве пользователя:
- Векторные эмбеддинги создаются локально
- Поиск происходит в SQLite базе данных
- Никакие сырые данные не покидают устройство

#### Автоматическая индексация
Система автоматически обрабатывает встречи в фоновом режиме:
- Разделение транскрипций на смысловые блоки
- Создание векторных представлений
- Индексирование по дате, теме, участникам

#### Глобальный поиск
Поддержка сложных запросов по всей истории:
- "Найди все обсуждения API-интерфейсов"
- "Покажи решения по проблеме с аутентификацией"
- "Что мы решили на встрече 15 марта?"

### Интерпретация визуального контента

#### Анализ скриншотов
Приложение может анализировать визуальный контент:
- Слайды презентаций
- Блоки кода
- Диаграммы и схемы
- Технические документы

#### Интеграция с ИИ
Скриншоты передаются в ИИ-модели для анализа:
- Объяснение сложных концепций
- Поиск ошибок в коде
- Предложения по улучшению дизайна
- Ответы на вопросы по визуальному материалу

### Режимы работы ИИ

#### Live Assistance
Работа в реальном времени во время встречи:
- Мгновенные подсказки по формулировке ответов
- Анализ технических вопросов
- Генерация контраргументов
- Поиск информации в истории

#### Manual Mode
Ручной режим для частных запросов:
- Возможность задать вопрос ИИ через микрофон
- Получение ответа без участия в основной беседе
- Подготовка сложных ответов

#### Recap & Summary
Автоматическая генерация итогов:
- Выделение ключевых моментов
- Формулировка решений
- Создание списка задач
- Генерация вопросов для уточнения

### Производительность и задержки

Система оптимизирована для работы в реальном времени:
- Задержка взаимодействий менее 500 мс
- Параллельная обработка аудио, видео и текстовых запросов
- Эффективное использование ресурсов даже на минимальных конфигурациях

Эта комплексная система ИИ делает Natively мощным инструментом для профессиональных ситуаций, сочетающим скорость облачных моделей с приватностью локальной обработки.

